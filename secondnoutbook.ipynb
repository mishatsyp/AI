{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9481739,"sourceType":"datasetVersion","datasetId":5767536}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-27T19:33:50.445821Z","iopub.execute_input":"2024-09-27T19:33:50.446997Z","iopub.status.idle":"2024-09-27T19:33:54.174305Z","shell.execute_reply.started":"2024-09-27T19:33:50.446930Z","shell.execute_reply":"2024-09-27T19:33:54.173529Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import required libraries:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Set our random seed:\nSEED = 17","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:33:57.030511Z","iopub.execute_input":"2024-09-27T19:33:57.031276Z","iopub.status.idle":"2024-09-27T19:33:59.643467Z","shell.execute_reply.started":"2024-09-27T19:33:57.031235Z","shell.execute_reply":"2024-09-27T19:33:59.642524Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/kagglecomp/train.csv')[['id', 'text', 'target']]\ntest_df = pd.read_csv('/kaggle/input/kagglecomp/test.csv')[['id', 'text']]","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:34:03.829505Z","iopub.execute_input":"2024-09-27T19:34:03.830105Z","iopub.status.idle":"2024-09-27T19:34:03.914197Z","shell.execute_reply.started":"2024-09-27T19:34:03.830064Z","shell.execute_reply":"2024-09-27T19:34:03.913316Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Инициализация токенизатора и модели BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:01:29.045671Z","iopub.execute_input":"2024-09-27T20:01:29.046617Z","iopub.status.idle":"2024-09-27T20:01:29.505981Z","shell.execute_reply.started":"2024-09-27T20:01:29.046567Z","shell.execute_reply":"2024-09-27T20:01:29.505265Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Проверка доступности GPU\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:01:31.973656Z","iopub.execute_input":"2024-09-27T20:01:31.974505Z","iopub.status.idle":"2024-09-27T20:01:32.122464Z","shell.execute_reply.started":"2024-09-27T20:01:31.974463Z","shell.execute_reply":"2024-09-27T20:01:32.121574Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        text = str(self.data.iloc[idx]['text'])\n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        input_ids = inputs['input_ids'].squeeze(0)\n        attention_mask = inputs['attention_mask'].squeeze(0)\n        \n        if 'target' in self.data.columns:\n            label = torch.tensor(self.data.iloc[idx]['target'], dtype=torch.long)\n            return input_ids, attention_mask, label\n        else:\n            return input_ids, attention_mask","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:01:36.162402Z","iopub.execute_input":"2024-09-27T20:01:36.162797Z","iopub.status.idle":"2024-09-27T20:01:36.170679Z","shell.execute_reply.started":"2024-09-27T20:01:36.162758Z","shell.execute_reply":"2024-09-27T20:01:36.169733Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# Создание DataLoader для обучения и валидации\ntrain_dataset = TweetDataset(train_data, tokenizer)\nval_dataset = TweetDataset(val_data, tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:01:39.100848Z","iopub.execute_input":"2024-09-27T20:01:39.101202Z","iopub.status.idle":"2024-09-27T20:01:39.109612Z","shell.execute_reply.started":"2024-09-27T20:01:39.101167Z","shell.execute_reply":"2024-09-27T20:01:39.108717Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-6)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:01:42.423331Z","iopub.execute_input":"2024-09-27T20:01:42.424061Z","iopub.status.idle":"2024-09-27T20:01:42.431587Z","shell.execute_reply.started":"2024-09-27T20:01:42.424017Z","shell.execute_reply":"2024-09-27T20:01:42.430713Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, device, epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss, total_val_loss = 0, 0\n        correct_predictions = 0\n\n        # Обучение\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n\n        # Валидация\n        model.eval()\n        for batch in val_loader:\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            with torch.no_grad():\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                total_val_loss += outputs.loss.item()\n                preds = torch.argmax(outputs.logits, dim=1)\n                correct_predictions += torch.sum(preds == labels).item()\n\n        accuracy = correct_predictions / len(val_loader.dataset)\n        print(f'Epoch {epoch + 1}, Train Loss: {total_loss / len(train_loader)}, Val Loss: {total_val_loss / len(val_loader)}, Val Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:01:44.749353Z","iopub.execute_input":"2024-09-27T20:01:44.750212Z","iopub.status.idle":"2024-09-27T20:01:44.759157Z","shell.execute_reply.started":"2024-09-27T20:01:44.750169Z","shell.execute_reply":"2024-09-27T20:01:44.758181Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, device)\n\n# Подготовка данных для предсказаний на тестовой выборке\ntest_dataset = TweetDataset(test_df, tokenizer)\ntest_loader = DataLoader(test_dataset, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:01:47.768107Z","iopub.execute_input":"2024-09-27T20:01:47.769000Z","iopub.status.idle":"2024-09-27T20:05:55.161712Z","shell.execute_reply.started":"2024-09-27T20:01:47.768956Z","shell.execute_reply":"2024-09-27T20:05:55.160735Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 0.6364290918421558, Val Loss: 0.5316555071622133, Val Accuracy: 0.7859487852921865\nEpoch 2, Train Loss: 0.4783213500745027, Val Loss: 0.4353235368616879, Val Accuracy: 0.8220617202889035\nEpoch 3, Train Loss: 0.39661263371389993, Val Loss: 0.40621344139799476, Val Accuracy: 0.8312541037426132\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\npredictions = []\nfor batch in test_loader:\n    input_ids, attention_mask = [b.to(device) for b in batch]\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:06:42.063830Z","iopub.execute_input":"2024-09-27T20:06:42.064204Z","iopub.status.idle":"2024-09-27T20:06:56.724647Z","shell.execute_reply.started":"2024-09-27T20:06:42.064168Z","shell.execute_reply":"2024-09-27T20:06:56.723763Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test_df['id'], 'target': predictions})\nsubmission.to_csv('submission_bert.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:07:04.806589Z","iopub.execute_input":"2024-09-27T20:07:04.806953Z","iopub.status.idle":"2024-09-27T20:07:04.831071Z","shell.execute_reply.started":"2024-09-27T20:07:04.806917Z","shell.execute_reply":"2024-09-27T20:07:04.830039Z"},"trusted":true},"execution_count":39,"outputs":[]}]}