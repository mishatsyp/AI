{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Bert + catboost**","metadata":{}},{"cell_type":"code","source":"!pip install catboost","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:34:35.636159Z","iopub.execute_input":"2024-09-27T08:34:35.636960Z","iopub.status.idle":"2024-09-27T08:34:48.066233Z","shell.execute_reply.started":"2024-09-27T08:34:35.636912Z","shell.execute_reply":"2024-09-27T08:34:48.065060Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.7)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.7.5)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.26.4)\nRequirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (2.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.14.1)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.22.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.1.2)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom catboost import CatBoostClassifier, Pool, metrics, cv\nfrom tqdm import tqdm\nimport numpy as np\nimport re\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport wandb\nfrom transformers import get_cosine_schedule_with_warmup\n\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:33:29.156078Z","iopub.execute_input":"2024-09-27T19:33:29.156762Z","iopub.status.idle":"2024-09-27T19:33:29.163386Z","shell.execute_reply.started":"2024-09-27T19:33:29.156721Z","shell.execute_reply":"2024-09-27T19:33:29.162247Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Скачиваем и обрабатываем данные\ntrain_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')  \ntrain_data = train_data[['text', 'target']] \n\n\ntrain_data.loc[train_data['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'To fight bioterrorism sir.', 'target'] = 1\ntrain_data.loc[train_data['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target'] = 1\ntrain_data.loc[train_data['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target'] = 1\ntrain_data.loc[train_data['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target'] = 0\ntrain_data.loc[train_data['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target'] = 0\ntrain_data.loc[train_data['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"Caution: breathing may be hazardous to your health.\", 'target'] = 0\ntrain_data.loc[train_data['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target'] = 1\ntrain_data.loc[train_data['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target'] = 0\n\n\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:33:56.929846Z","iopub.execute_input":"2024-09-27T19:33:56.930730Z","iopub.status.idle":"2024-09-27T19:33:57.048652Z","shell.execute_reply.started":"2024-09-27T19:33:56.930688Z","shell.execute_reply":"2024-09-27T19:33:57.047672Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Bert(nn.Module):\n    def __init__(self, trainable = False):\n        super().__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.bert = AutoModel.from_pretrained('distilbert-base-uncased').to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n        if trainable == False:\n            for m in self.bert.modules():\n                for name, params in m.named_parameters():\n                    params.requires_grad = False\n        self.target_indx = 0\n        \n\n    def forward(self, input):\n\n        encoding = self.tokenizer.batch_encode_plus(\n            input,  # List of input texts\n            padding=\"max_length\",\n            max_length=512,  # Pad to the maximum sequence length\n            truncation=True,  # Truncate to the maximum sequence length if necessary\n            return_tensors='pt',  # Return PyTorch tensors\n            add_special_tokens=True  # Add special tokens CLS and SEP\n        )\n\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        out = self.bert(input_ids, attention_mask, output_hidden_states=True)\n        out = out[0][:,self.target_indx,:]\n\n        return out\n    \nclass dataset_bilder(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n    \n    \nclass bert_catboost(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.bert = Bert()\n        self.catboost = CatBoostClassifier(**params)\n        \n    def train_catboost(self, train, val, batch_train_size, batch_val_size):\n        bert_output = []\n        bert_output_val = []\n        target = []\n        target_val = []\n        \n        for X_train, y_train in tqdm(train, desc=\"Train\", colour=\"CYAN\"):\n            if y_train.shape[0] == batch_train_size:\n                bert_output.append(self.bert(X_train).tolist())\n                target.append(y_train.tolist())\n            \n        for X_val, y_val in tqdm(val, desc=\"Val\", colour=\"CYAN\"):\n            if y_val.shape[0] == batch_val_size:\n                bert_output_val.append(self.bert(X_val).tolist())\n                target_val.append(y_val.tolist())\n        \n        bert_output = np.array(bert_output)\n        bert_output = bert_output.reshape(-1, bert_output.shape[-1])\n        bert_output_val = np.array(bert_output_val)\n        bert_output_val = bert_output_val.reshape(-1, bert_output_val.shape[-1])\n        \n        target = np.array(target)\n        target = target.reshape(-1)\n        target_val = np.array(target_val)\n        target_val = target_val.reshape(-1)\n        \n        self.catboost.fit(\n            bert_output, target,\n            eval_set=(bert_output_val, target_val),\n            #logging_level='Verbose',  # you can uncomment this for text output\n            plot=True\n        );\n        \n        \n    def forward(self, input):\n        output = self.bert(input).tolist()\n        output = self.catboost.predict(output)\n        \n        return output\n\n    \nclass model_testment(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        \n    def take_accuracy(self,test_data):\n        first_iter = True\n        for X, y in tqdm(test_data, desc=\"Testment\", colour=\"CYAN\"):\n            if first_iter == True:\n                model_prediction = self.model(X)\n                target = y.numpy() \n                first_iter = False\n            else:\n                model_prediction = np.concatenate((model_prediction, self.model(X)), axis = 0)\n                target = np.concatenate((target, y.numpy()), axis = 0)\n        \n        return len(model_prediction[model_prediction == target])/len(model_prediction)\n    \n    def create_result(self, pd_test):\n        texts = pd_test['text'].values.tolist()   \n        model_output = []\n        for text in texts:\n            model_output.append(self.model([text])[0])\n        model_output = np.array(model_output)\n        \n        submission = pd.DataFrame({'id': pd_test['id'], 'target': model_output})\n        \n        return submission\n    \n    def forward(self, input):\n        return self.model(input)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:57:19.916347Z","iopub.execute_input":"2024-09-27T08:57:19.917111Z","iopub.status.idle":"2024-09-27T08:57:19.940474Z","shell.execute_reply.started":"2024-09-27T08:57:19.917068Z","shell.execute_reply":"2024-09-27T08:57:19.939456Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split  # To split data into training and testing sets\n#Train-test split\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.text.values.tolist(), train_data.target.values.tolist(), test_size=0.2)\n\ntrain_dataset = dataset_bilder(X_train, y_train)\nval_dataset = dataset_bilder(X_val, y_val)\n\nbatch_train_size = 32\nbatch_val_size = 64\n\ntrain = DataLoader(\n        train_dataset,\n        batch_size=batch_train_size,\n        num_workers=4,\n        shuffle=True,\n        collate_fn=None,\n    )\n\nval = DataLoader(\n        val_dataset,\n        batch_size=batch_val_size,\n        num_workers=4,\n        shuffle=False,\n        collate_fn=None,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:57:20.882336Z","iopub.execute_input":"2024-09-27T08:57:20.882707Z","iopub.status.idle":"2024-09-27T08:57:20.895238Z","shell.execute_reply.started":"2024-09-27T08:57:20.882671Z","shell.execute_reply":"2024-09-27T08:57:20.894384Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nparams = {\n    'iterations': 500,\n    'learning_rate': 0.1,\n    'eval_metric': metrics.Accuracy(),\n    'random_seed': 42,\n    'logging_level': 'Silent',\n    'use_best_model': True,\n    'task_type' : 'GPU'\n}\n\nmodel = bert_catboost(params)\nmodel.train_catboost(train, val, batch_train_size, batch_val_size)\n\nacc = model_testment(model).take_accuracy(val)\nprint('accuracy: ' + str(acc))\n\nsubmission = model_testment(model).create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:57:21.741838Z","iopub.execute_input":"2024-09-27T08:57:21.742719Z","iopub.status.idle":"2024-09-27T09:00:00.572831Z","shell.execute_reply.started":"2024-09-27T08:57:21.742667Z","shell.execute_reply":"2024-09-27T09:00:00.571709Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Train: 100%|\u001b[36m██████████\u001b[0m| 188/188 [01:47<00:00,  1.76it/s]\nVal: 100%|\u001b[36m██████████\u001b[0m| 24/24 [00:26<00:00,  1.09s/it]\nWarning: less than 75% GPU memory available for training. Free: 11138.125 Total: 15095.0625\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88fe2a980ffb4a85a4f8445ea9486caf"}},"metadata":{}}]},{"cell_type":"code","source":"acc = model_testment(model).take_accuracy(val)\nprint('accuracy: ' + str(acc))","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:00:00.577587Z","iopub.execute_input":"2024-09-27T09:00:00.580147Z","iopub.status.idle":"2024-09-27T09:00:27.517612Z","shell.execute_reply.started":"2024-09-27T09:00:00.580096Z","shell.execute_reply":"2024-09-27T09:00:27.516393Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Testment: 100%|\u001b[36m██████████\u001b[0m| 24/24 [00:26<00:00,  1.12s/it]","output_type":"stream"},{"name":"stdout","text":"accuracy: 0.8073089700996677\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = model_testment(model).create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T08:38:58.628273Z","iopub.execute_input":"2024-09-27T08:38:58.628676Z","iopub.status.idle":"2024-09-27T08:40:02.304212Z","shell.execute_reply.started":"2024-09-27T08:38:58.628638Z","shell.execute_reply":"2024-09-27T08:40:02.303194Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#  **DistilBertForSequenceClassification**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass Bert_classification(nn.Module): # создаем класс с бертом и токенайзером\n    def __init__(self):\n        super().__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n    def forward(self, input):\n        encoding = self.tokenizer.batch_encode_plus(\n            input,  # List of input texts\n            padding=\"max_length\",\n            max_length=512,  # Pad to the maximum sequence length\n            truncation=True,  # Truncate to the maximum sequence length if necessary\n            return_tensors='pt',  # Return PyTorch tensors\n            add_special_tokens=False  # Add special tokens CLS and SEP\n        )\n\n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n\n        out = self.bert(input_ids, attention_mask, output_hidden_states=True)\n\n        return out.logits\n    \nclass dataset_bilder(Dataset): # для создания датасета\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n\n    \nclass model_usage(): # создем класс с кодом для обучения, тестирования и сохранению результата\n    def __init__(self, model):\n        self.model = model \n    \n    def train(self, dataloader, optimizer, loss_func, scheduler, epochs):\n        self.model.bert.train()\n        for _ in range(epochs):\n            for texts, labels in tqdm(dataloader, desc=\"Epoch\", colour=\"GREEN\"):\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                wandb.log({\"loss_val\": loss})\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n        \n\n    def test(self, dataloader, loss_func):\n        self.model.eval()\n        testloss, correct = 0, 0\n        num_batches = len(dataloader)\n        size = len(dataloader.dataset)\n        \n        with torch.no_grad():\n            for texts, labels in tqdm(dataloader, desc=\"Eval\", colour=\"CYAN\"):\n                labels = labels.to(device)\n                output = self.model(texts)\n                labels = labels.long()\n                loss = loss_func(output, labels)\n                testloss += loss.item()\n                preds = torch.argmax(output, dim=1)\n                correct += (preds == labels).type(torch.float).sum().item()\n\n        correct /= size\n        testloss /= num_batches\n\n        print(f\"Testment: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {testloss:>8f} \\n\")\n        \n    def create_result(self, pd_test):\n        texts = pd_test['text'].values.tolist()   \n        model_output = []\n        for text in tqdm(texts, desc=\"Creating\", colour=\"CYAN\"):\n            model_output.append(torch.argmax(self.model([text])).item())\n        model_output = np.array(model_output)\n        \n        submission = pd.DataFrame({'id': pd_test['id'], 'target': model_output})\n        \n        return submission","metadata":{"execution":{"iopub.status.busy":"2024-09-27T19:34:03.238556Z","iopub.execute_input":"2024-09-27T19:34:03.239479Z","iopub.status.idle":"2024-09-27T19:34:03.265222Z","shell.execute_reply.started":"2024-09-27T19:34:03.239413Z","shell.execute_reply":"2024-09-27T19:34:03.263909Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Сплитаем дату\nfrom sklearn.model_selection import train_test_split  # To split data into training and testing sets\n\nX_train, X_val, y_train, y_val = train_test_split(train_data.text.values.tolist(), train_data.target.values.tolist(), test_size=0.2)\n\n#Создаем даталоадер для обучения\ntrain_dataset = dataset_bilder(X_train, y_train)\nval_dataset = dataset_bilder(X_val, y_val)\n\nbatch_train_size = 32\nbatch_val_size = 64\n\ntrain = DataLoader(\n        train_dataset,\n        batch_size=batch_train_size,\n        num_workers=4,\n        shuffle=True,\n        collate_fn=None,\n    )\n\nval = DataLoader(\n        val_dataset,\n        batch_size=batch_val_size,\n        num_workers=4,\n        shuffle=False,\n        collate_fn=None,\n    )\n# Инициализируем модель\nmodel = Bert_classification()\n\n# Создаем вещи для обучения\nepochs = 5\noptimizer = optim.Adam(model.bert.parameters(), lr=5e-5)\nloss_func = nn.CrossEntropyLoss()\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer, num_warmup_steps=200, \n    num_training_steps= 191 * epochs  # 191 - число батчей в датасете\n)\n\nwandb.login()\nwandb.init(\n            project=\"DistilBertForSequenceClassification\",\n            config={\n                \"config\": 'Tuning',\n            })\n\n# Обучаем\nmodel_f = model_usage(model)\nmodel_f.train(train, optimizer, loss_func, scheduler, epochs)                            \nBert_model = model_f.model\n\n# Тестируем\nmodel_f.test(val, loss_func)\n\n# Сохраняем результат на тест дате\nsubmission = model_f.create_result(test_data)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-27T20:15:41.712683Z","iopub.execute_input":"2024-09-27T20:15:41.713611Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:vrjv9bfa) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a99515340814ac7aa4fa6dd75dfe4ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_val</td><td>▆▆▄▆▃█▅▄▄▄▃▂▂▂▃▃▁▄▂▃▂▃▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_val</td><td>0.18071</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">zany-wildflower-15</strong> at: <a href='https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification/runs/vrjv9bfa' target=\"_blank\">https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification/runs/vrjv9bfa</a><br/> View project at: <a href='https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification' target=\"_blank\">https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240927_193436-vrjv9bfa/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:vrjv9bfa). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240927_201542-l4neym17</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification/runs/l4neym17' target=\"_blank\">whole-yogurt-16</a></strong> to <a href='https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification' target=\"_blank\">https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification/runs/l4neym17' target=\"_blank\">https://wandb.ai/pretzandalf_projects/DistilBertForSequenceClassification/runs/l4neym17</a>"},"metadata":{}},{"name":"stderr","text":"Epoch:  59%|\u001b[32m█████▊    \u001b[0m| 112/191 [02:34<01:48,  1.38s/it]","output_type":"stream"}]}]}